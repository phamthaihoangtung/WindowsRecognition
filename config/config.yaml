logging:
  use_wandb: true  # Set to false to disable wandb logging
  wandb_project: "WindowsRecognition"
  wandb_experiment: "base_model_b4"  # Experiment name for wandb
  wandb_mode: "online"  # Set to "online" or "offline"

paths:
  train_images: "data/processed/v3/images/train"
  train_masks: "data/processed/v3/annotations/train"
  val_images: 
    - "data/processed/v3/images/val"  # Example of multiple validation sets
    # - "data/processed/v3/images/val"  # Single or multiple validation sets
  val_masks: 
    - "data/processed/v3/annotations/val"  # Example of multiple validation masks
    # - "data/processed/v3/annotations/val"  # Single or multiple validation masks
  test_images: "data/processed/v3/images/test"
  test_masks: "data/processed/v3/annotations/test"
  # model_save_path: "models/fpn_multiscale_sobel_loss.ckpt"  # Use experiment name for model save path

hyperparameters:
  train_batch_size: 4  # Separate train batch size
  val_batch_size: 4    # Separate val batch size
  epochs: 60
  learning_rate: 0.001
  image_size: [512, 512]  # Configurable image size as [height, width]
  enable_early_stopping: true  # Enable or disable early stopping
  early_stopping_patience: 10  # Number of epochs with no improvement after which training will be stopped
  scheduler:
    type: "ExponentialLR"
    # step_size: 5
    gamma: 0.925 # Decay rate for ExponentialLR

model:
  name: "Unet"
  backbone: "efficientnet-b4"  # Backbone for the model
  encoder_weights: "imagenet"  # [imagenet, null] Set to null to not use pre-trained weights 
  in_channels: 3
  classes: 1

trainer:
  accelerator: "gpu"  # Use "gpu" for GPU training or "cpu"
  devices: 1          # Number of GPUs to use (set to None for CPU)

inference:
  model_path: "models/base_model_b4/last.ckpt"  # Path to the model checkpoint for inference
  refined_segmentation_mode: "contour"
  use_tta: false  # Use test-time augmentation